{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_match(algorithm,query_img,train_img,min_match_count,verbose=False):\n",
    "    query_image = query_img\n",
    "    train_image = train_img\n",
    "    key_pts1,des1 = algorithm(query_image,None)\n",
    "    key_pts2,des2 = algorithm(train_image,None)\n",
    "    msed=np.inf\n",
    "    if not (isinstance(des1,np.float32)&isinstance(des2,np.float32)):\n",
    "        des1 = np.float32(des1)\n",
    "        des2 = np.float32(des2)\n",
    "    \n",
    "    flann_idx = 1\n",
    "    index_params = dict(algorithm = flann_idx, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = [m for m,n in matches if m.distance < 0.7*n.distance]\n",
    "    \n",
    "    if len(good)>min_match_count:\n",
    "        src_pts = np.float32([ key_pts1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ key_pts2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        if M is None:\n",
    "            return [0],[0],[0],[0],np.inf\n",
    "        #print(M,mask)\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "        h,w = query_image.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        dst = cv2.perspectiveTransform(pts,M)\n",
    "        msed = np.mean([np.sqrt(np.sum(diff)) for diff in (np.power(pts-dst,2))]/(np.sqrt(h**2+w**2)))\n",
    "        #cv2.polylines(train_image,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "    else:\n",
    "        if verbose==True:\n",
    "            print( \"Not enough matches are found - {}/{}\".format(len(good), min_match_count) )\n",
    "        matchesMask = [0]\n",
    "    \n",
    "    return key_pts1, key_pts2, good, matchesMask, msed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_dict=dict(sift=cv2.xfeatures2d.SIFT_create().detectAndCompute,\n",
    "              surf=cv2.xfeatures2d.SURF_create(400).detectAndCompute)\n",
    "min_matches=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder1 = os.path.join(os.curdir,'dayKvadr')\n",
    "data_set1 = [os.path.join(data_folder1,file) for file in os.listdir(data_folder1) if os.path.isfile(os.path.join(data_folder1,file))]\n",
    "data_folder2 = os.path.join(os.curdir,'dayPony')\n",
    "data_set2 = [os.path.join(data_folder2,file) for file in os.listdir(data_folder2) if os.path.isfile(os.path.join(data_folder2,file))]\n",
    "data_folder3 = os.path.join(os.curdir,'nightKvadr')\n",
    "data_set3 = [os.path.join(data_folder3,file) for file in os.listdir(data_folder3) if os.path.isfile(os.path.join(data_folder3,file))]\n",
    "data_folder4 = os.path.join(os.curdir,'nightPony')\n",
    "data_set4 = [os.path.join(data_folder4,file) for file in os.listdir(data_folder4) if os.path.isfile(os.path.join(data_folder4,file))]\n",
    "data_folder5 = os.path.join(os.curdir,'train')\n",
    "data_set5 = [os.path.join(data_folder5,file) for file in os.listdir(data_folder5) if os.path.isfile(os.path.join(data_folder5,file))]\n",
    "data_folder6 = os.path.join(os.curdir,'test')\n",
    "data_set6 = [os.path.join(data_folder6,file) for file in os.listdir(data_folder6) if os.path.isfile(os.path.join(data_folder6,file))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [cv2.imread(image,0) for image in data_set5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_matches_train = dict()\n",
    "\n",
    "inliers_matches_train = [[] for i in range(len(train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ed/opt/anaconda3/envs/newEnv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for alg_name, alg in alg_dict.items():\n",
    "    for im_i_idx, image_i in enumerate(train):\n",
    "        _, __, match, inlier ,msd = detect_match(algorithm=alg,query_img=image_i,train_img=train[5],\n",
    "                                                     min_match_count=10)\n",
    "        if match==0:\n",
    "            match=np.inf\n",
    "        inliers_matches_train[im_i_idx].append(np.sum(inlier)/(len(match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write result column for train dataset\n",
    "Y_train = []\n",
    "for i in range(len(data_set5)):\n",
    "    if(int((data_set5[i].split(\"_\")[1]).split(\".\")[0]) >= 3210):\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        Y_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all NaN to 0.0\n",
    "for i in range(len(inliers_matches_train)):\n",
    "    for j in range(len(inliers_matches_train[i])):\n",
    "        if(math.isnan(inliers_matches_train[i][j])): \n",
    "            inliers_matches_train[i][j] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [cv2.imread(image,0) for image in data_set6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_matches_test = dict()\n",
    "inliers_matches_test = [[] for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ed/opt/anaconda3/envs/newEnv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for alg_name, alg in alg_dict.items():\n",
    "    for im_i_idx, image_i in enumerate(test):\n",
    "        _, __, match, inlier ,msd = detect_match(algorithm=alg,query_img=image_i,train_img=test[2],\n",
    "                                                     min_match_count=10)\n",
    "        if match==0:\n",
    "            match=np.inf\n",
    "        inliers_matches_test[im_i_idx].append(np.sum(inlier)/(len(match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all NaN to 0.0\n",
    "for i in range(len(inliers_matches_test)):\n",
    "    for j in range(len(inliers_matches_test[i])):\n",
    "        if(math.isnan(inliers_matches_test[i][j])): \n",
    "            inliers_matches_test[i][j] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write result column for test dataset\n",
    "Y_test = []\n",
    "for i in range(len(data_set6)):\n",
    "    num = int((data_set5[i].split(\"_\")[1]).split(\".\")[0])\n",
    "    if(((num >= 3216) and (num <= 3336)) or (num >= 3617)):\n",
    "        Y_test.append(1)\n",
    "    else:\n",
    "        Y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  6]\n",
      " [ 2 12]]\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier().fit(np.array(inliers_matches_train), Y_train)\n",
    "predictions = xgb_model.predict(np.array(inliers_matches_test))\n",
    "actuals = Y_test\n",
    "print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i] == Y_test[i]):\n",
    "        count = count + 1\n",
    "accuracy = count/len(predictions)\n",
    "print(accuracy)\n",
    "# It is better to use metrics from sklearn but I am tired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have 71% acuracy, \n",
    "# actualy train and test data set pretty small\n",
    "# Also it is only 2 features in DS affect to our model \n",
    "# not in best side. By the way it will be nice to use \n",
    "# KFold for better training and GridSearchCV to find best\n",
    "# params for XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
